{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from konlpy.tag import Kkma\n",
    "kor_tagger = Kkma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch = torch.Tensor([0.6,-0.2,0.7,0.3,0.7,-0.2,0.1,0.1])\n",
    "tensorflow = torch.Tensor([0.4,-0.1,0.6,-0.2,0.6,-0.2,0.3,0.4])\n",
    "cat = torch.Tensor([-0.3,0.2,0.1,0.2,-0.2,0.1,-0.3,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1500000953674316"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch.dot(tensorflow) # inner product = word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26999998092651367"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch.dot(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns cosine similarity between x1 and x2, computed along dim.<br/>\n",
    "\n",
    "\\\\(similarity= \\frac{x_{1}\\cdot x_{2}}{max(\\left\\|x_{1}\\right\\|_{2}\\cdot\\left\\|x_2\\right\\|_2,\\epsilon)}\\\\)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Parameters:\t<br/>\n",
    "- x1 (Tensor) – First input.<br/>\n",
    "- x2 (Tensor) – Second input (of size matching x1).<br/>\n",
    "- dim (int, optional) – Dimension of vectors. Default: 1<br/>\n",
    "- eps (float, optional) – Small value to avoid division by zero. Default: 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.8417\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(pytorch.view(1,-1),tensorflow.view(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.3800\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(pytorch.view(1,-1),cat.view(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Embedding() \n",
    "##### torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None)\n",
    "\n",
    "A simple lookup table that stores embeddings of a fixed dictionary and size.<br/>\n",
    "This module is often used to store word embeddings and retrieve them using indices. <br/>The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "> Parameters:\t\n",
    "- num_embeddings (int) – size of the dictionary of embeddings\n",
    "- embedding_dim (int) – the size of each embedding vector\n",
    "- padding_idx (int, optional) – If given, pads the output with the embedding vector at - padding_idx (initialized to zeros) whenever it encounters the index.\n",
    "- max_norm (float, optional) – If given, will renormalize the embeddings to always have a norm lesser than this\n",
    "- norm_type (float, optional) – The p of the p-norm to compute for the max_norm option\n",
    "- scale_grad_by_freq (bool, optional) – if given, this will scale gradients by the frequency of the words in the mini-batch.\n",
    "- sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.\n",
    "\n",
    "Output: (*, embedding_dim), where * is the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 2.1016 -1.0317  0.1484  0.1075  1.0484\n",
       "-0.6429  0.8057 -0.3392 -0.1415 -1.1187\n",
       " 0.6939  0.4873 -1.1391 -0.5029  0.7813\n",
       "-0.2998 -0.7556  0.4820 -0.8570  0.1664\n",
       "-0.6996 -0.0582  1.9101  0.6987  1.2526\n",
       " 1.4845 -0.3605 -0.8905 -0.9029  0.8305\n",
       "-0.3070 -1.1148 -0.0758 -2.7965 -0.0669\n",
       "[torch.FloatTensor of size 7x5]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(7,5) # 총 단어 갯수, 임베딩 시킬 차원\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6939  0.4873 -1.1391 -0.5029  0.7813\n",
       "[torch.FloatTensor of size 1x5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = torch.LongTensor([2]) # 2은 사과의 인덱스\n",
    "embed(Variable(apple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.6429  0.8057 -0.3392 -0.1415 -1.1187\n",
       "-0.2998 -0.7556  0.4820 -0.8570  0.1664\n",
       " 1.4845 -0.3605 -0.8905 -0.9029  0.8305\n",
       "[torch.FloatTensor of size 3x5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = torch.LongTensor([1,3,5]) # 단어의 시퀀스(문장)\n",
    "embed(Variable(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.EmbeddingBag "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 워드 벡터를 합하거나 평균해서 하나의 벡터를 반환 ==> Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.1083 -1.0966 -0.0888  0.5519  0.1315\n",
       "[torch.FloatTensor of size 1x5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_sum = nn.EmbeddingBag(7, 5, mode='mean') # mean or sum\n",
    "sentence = Variable(torch.LongTensor([[1,3,5]])) # 토큰 1,3,5로 이루어진 문장\n",
    "embedding_sum(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음의 코퍼스를 형태소로 tokenize하고 Vocabulary를 구축하라(Word2index)\n",
    "- 각 단어를 10차원으로 임베딩하기 위한 nn.Embedding을 선언하라\n",
    "- \"토치\"와 \"텐서플로우\"라는 단어를 Embedding matrix에서 읽어서 두 벡터를 내적하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"안녕하세요 저는 파이토치를 공부중입니다.\",\"파이토치는 딥러닝 라이브러리이다\", \"파이토치와 유사한 것으로 텐서플로우와 케라스 등이 있다\",\n",
    "         \"파이토치는 정말 쉽다\",\"텐서플로우는 구글이 만들었다\",\"페북에서는 파이토치를 만들었다\", \"파이썬과 쉽게 사용할 수 있는 것이 장점이다\",\n",
    "         \"그 중 특히 자연어처리할 때 파이토치가 좋다\",\"하지만 아직 베타 버전인게 파이토치의 단점이다\",\"원래는 루아라는 언어로 만들어진 토치라는 프레임워크였다\",\n",
    "         \"텐서플로우나 파이토치는 자동미분 기능을 제공한다\", \"이를 이용하면 딥러닝 모델을 쉽게 만들 수 있다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [kor_tagger.morphs(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['안녕', '하', '세요', '저', '는', '파이', '토치', '를', '공부', '중', '이', 'ㅂ니다', '.'],\n",
       " ['파이', '토치', '는', '딥', '러닝', '라이브러리', '이', '다'],\n",
       " ['파이',\n",
       "  '토치',\n",
       "  '와',\n",
       "  '유사',\n",
       "  '하',\n",
       "  'ㄴ',\n",
       "  '것',\n",
       "  '으로',\n",
       "  '텐서플로우',\n",
       "  '와',\n",
       "  '하',\n",
       "  '게',\n",
       "  '라스',\n",
       "  '등',\n",
       "  '이',\n",
       "  '있',\n",
       "  '다'],\n",
       " ['파이', '토치', '는', '정말', '쉽', '다'],\n",
       " ['텐서플로우', '는', '구', '글', '이', '만들', '었', '다'],\n",
       " ['페북에서', '는', '파이', '토치', '를', '만들', '었', '다'],\n",
       " ['파이',\n",
       "  '썰',\n",
       "  'ㄴ',\n",
       "  '과',\n",
       "  '쉽',\n",
       "  '게',\n",
       "  '사용',\n",
       "  '하',\n",
       "  'ㄹ',\n",
       "  '수',\n",
       "  '있',\n",
       "  '는',\n",
       "  '것',\n",
       "  '이',\n",
       "  '장점',\n",
       "  '이',\n",
       "  '다'],\n",
       " ['그', '중', '특히', '자연어', '처리', '하', 'ㄹ', '때', '파이', '토치', '가', '좋', '다'],\n",
       " ['하지만',\n",
       "  '아직',\n",
       "  '베타',\n",
       "  '버전',\n",
       "  '이',\n",
       "  'ㄴ',\n",
       "  '것',\n",
       "  '이',\n",
       "  '파이',\n",
       "  '토치',\n",
       "  '의',\n",
       "  '단점',\n",
       "  '이',\n",
       "  '다'],\n",
       " ['원래',\n",
       "  '는',\n",
       "  '루',\n",
       "  '아',\n",
       "  '이',\n",
       "  '라는',\n",
       "  '언어',\n",
       "  '로',\n",
       "  '만들',\n",
       "  '어',\n",
       "  '지',\n",
       "  'ㄴ',\n",
       "  '토치',\n",
       "  '이',\n",
       "  '라는',\n",
       "  '프레임',\n",
       "  '워크',\n",
       "  '이',\n",
       "  '었',\n",
       "  '다'],\n",
       " ['텐서플로우', '나', '파이', '토치', '는', '자동', '미분', '기능', '을', '제공', '하', 'ㄴ다'],\n",
       " ['이르',\n",
       "  'ㄹ',\n",
       "  '이용',\n",
       "  '하',\n",
       "  '면',\n",
       "  '딥',\n",
       "  '러닝',\n",
       "  '모델',\n",
       "  '을',\n",
       "  '쉽',\n",
       "  '게',\n",
       "  '만들',\n",
       "  'ㄹ',\n",
       "  '수',\n",
       "  '있',\n",
       "  '다']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={}\n",
    "for sent in tokenized:\n",
    "    for word in sent:\n",
    "        if word2index.get(word)==None:\n",
    "            word2index[word]=len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'안녕': 0,\n",
       " '하': 1,\n",
       " '세요': 2,\n",
       " '저': 3,\n",
       " '는': 4,\n",
       " '파이': 5,\n",
       " '토치': 6,\n",
       " '를': 7,\n",
       " '공부': 8,\n",
       " '중': 9,\n",
       " '이': 10,\n",
       " 'ㅂ니다': 11,\n",
       " '.': 12,\n",
       " '딥': 13,\n",
       " '러닝': 14,\n",
       " '라이브러리': 15,\n",
       " '다': 16,\n",
       " '와': 17,\n",
       " '유사': 18,\n",
       " 'ㄴ': 19,\n",
       " '것': 20,\n",
       " '으로': 21,\n",
       " '텐서플로우': 22,\n",
       " '게': 23,\n",
       " '라스': 24,\n",
       " '등': 25,\n",
       " '있': 26,\n",
       " '정말': 27,\n",
       " '쉽': 28,\n",
       " '구': 29,\n",
       " '글': 30,\n",
       " '만들': 31,\n",
       " '었': 32,\n",
       " '페북에서': 33,\n",
       " '썰': 34,\n",
       " '과': 35,\n",
       " '사용': 36,\n",
       " 'ㄹ': 37,\n",
       " '수': 38,\n",
       " '장점': 39,\n",
       " '그': 40,\n",
       " '특히': 41,\n",
       " '자연어': 42,\n",
       " '처리': 43,\n",
       " '때': 44,\n",
       " '가': 45,\n",
       " '좋': 46,\n",
       " '하지만': 47,\n",
       " '아직': 48,\n",
       " '베타': 49,\n",
       " '버전': 50,\n",
       " '의': 51,\n",
       " '단점': 52,\n",
       " '원래': 53,\n",
       " '루': 54,\n",
       " '아': 55,\n",
       " '라는': 56,\n",
       " '언어': 57,\n",
       " '로': 58,\n",
       " '어': 59,\n",
       " '지': 60,\n",
       " '프레임': 61,\n",
       " '워크': 62,\n",
       " '나': 63,\n",
       " '자동': 64,\n",
       " '미분': 65,\n",
       " '기능': 66,\n",
       " '을': 67,\n",
       " '제공': 68,\n",
       " 'ㄴ다': 69,\n",
       " '이르': 70,\n",
       " '이용': 71,\n",
       " '면': 72,\n",
       " '모델': 73}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = nn.Embedding(len(word2index),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['러닝']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  14\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  42\n",
       " [torch.LongTensor of size 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_tensor = Variable(torch.LongTensor([word2index['러닝']]))\n",
    "tensorflow_tensor = Variable(torch.LongTensor([word2index['자연어']]))\n",
    "pytorch_tensor, tensorflow_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.7360  0.0456  1.3025  0.3129  0.2624  1.8678  1.0026 -0.5596 -0.3155 -0.4390\n",
       " [torch.FloatTensor of size 1x10], Variable containing:\n",
       " -1.0361  0.3765 -0.0546 -0.1214  0.3796 -1.2015  0.9841  1.5352 -0.9129  0.2730\n",
       " [torch.FloatTensor of size 1x10])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_vector = matrix(pytorch_tensor)\n",
    "tensorflow_vector = matrix(tensorflow_tensor)\n",
    "pytorch_vector,tensorflow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "print(pytorch_vector.size(),tensorflow_vector.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.7035\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_vector.dot(tensorflow_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.3715\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(pytorch_vector,tensorflow_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
