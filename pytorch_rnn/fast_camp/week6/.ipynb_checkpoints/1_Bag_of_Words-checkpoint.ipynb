{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from konlpy.tag import Kkma\n",
    "kor_tagger = Kkma()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\a/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\cosmos\\\\envs\\\\pytorch\\\\nltk_data'\n    - 'C:\\\\cosmos\\\\envs\\\\pytorch\\\\share\\\\nltk_data'\n    - 'C:\\\\cosmos\\\\envs\\\\pytorch\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\a\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fbbe6ff10e8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hi, my name is sungdong. What's your name?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\cosmos\\envs\\pytorch\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m--> 128\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[1;32mC:\\cosmos\\envs\\pytorch\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\cosmos\\envs\\pytorch\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\cosmos\\envs\\pytorch\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\cosmos\\envs\\pytorch\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 675\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - 'C:\\\\Users\\\\a/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\cosmos\\\\envs\\\\pytorch\\\\nltk_data'\n    - 'C:\\\\cosmos\\\\envs\\\\pytorch\\\\share\\\\nltk_data'\n    - 'C:\\\\cosmos\\\\envs\\\\pytorch\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\a\\\\AppData\\\\Roaming\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "token = nltk.word_tokenize(\"Hi, my name is sungdong. What's your name?\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '하', '세요', '!', '저', '는', '파이', '토치', '를', '공부', '하', '는', '중', '이', 'ㅂ니다', '.']\n"
     ]
    }
   ],
   "source": [
    "token = kor_tagger.morphs(\"안녕하세요! 저는 파이토치를 공부하는 중입니다.\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕',\n",
       " '하',\n",
       " '세요',\n",
       " '!',\n",
       " '저',\n",
       " '는',\n",
       " '파이',\n",
       " '토치',\n",
       " '를',\n",
       " '공부',\n",
       " '하',\n",
       " '는',\n",
       " '중',\n",
       " '이',\n",
       " 'ㅂ니다',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'안녕': 0, '하': 1, '세요': 2, '!': 3, '저': 4, '는': 5, '파이': 6, '토치': 7, '를': 8, '공부': 9, '중': 10, '이': 11, 'ㅂ니다': 12, '.': 13}\n"
     ]
    }
   ],
   "source": [
    "word2index={} # dictionary for indexing\n",
    "for vo in token:\n",
    "    if word2index.get(vo) == None:\n",
    "        word2index[vo]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word,word2index):\n",
    "    tensor = torch.zeros(len(word2index))\n",
    "    index = word2index[word] \n",
    "    tensor[index]=1.\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 14]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch_vector = one_hot_encoding(\"토치\",word2index)\n",
    "print(torch_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_vector = one_hot_encoding(\"파이\",word2index)\n",
    "py_vector.dot(torch_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [[\"배고프다 밥줘\",\"FOOD\"],\n",
    "              [\"뭐 먹을만한거 없냐\",\"FOOD\"],\n",
    "              [\"맛집 추천\",\"FOOD\"],\n",
    "              [\"이 근처 맛있는 음식점 좀\",\"FOOD\"],\n",
    "              [\"밥줘\",\"FOOD\"],\n",
    "              [\"뭐 먹지?\",\"FOOD\"],\n",
    "              [\"삼겹살 먹고싶어\",\"FOOD\"],\n",
    "              [\"영화 보고싶다\",\"MEDIA\"],\n",
    "              [\"요즘 볼만한거 있어?\",\"MEDIA\"],\n",
    "              [\"영화나 예능 추천\",\"MEDIA\"],\n",
    "              [\"재밌는 드라마 보여줘\",\"MEDIA\"],\n",
    "              [\"신과 함께 줄거리 좀 알려줘\",\"MEDIA\"],\n",
    "              [\"고등랩퍼 다시보기 좀\",\"MEDIA\"],\n",
    "              [\"재밌는 영상 하이라이트만 보여줘\",\"MEDIA\"]]\n",
    "\n",
    "test_data = [[\"쭈꾸미 맛집 좀 찾아줘\",\"FOOD\"],\n",
    "             [\"매콤한 떡볶이 먹고싶다\",\"FOOD\"],\n",
    "             [\"강남 씨지비 조조 영화 스케줄표 좀\",\"MEDIA\"],\n",
    "             [\"효리네 민박 보고싶엉\",\"MEDIA\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y = list(zip(*train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip([1, 2, 3], \n",
    "         [4, 5, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [kor_tagger.morphs(x) for x in train_X] # Tokenize morphemes 형태소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['배고프', '다', '밥', '주', '어'],\n",
       " ['뭐', '먹', '을', '만하', 'ㄴ', '거', '없', '냐'],\n",
       " ['맛', '집', '추천'],\n",
       " ['이', '근처', '맛있', '는', '음식', '점', '좀'],\n",
       " ['밥', '주', '어'],\n",
       " ['뭐', '먹', '지', '?'],\n",
       " ['삼겹살', '먹', '고', '싶', '어'],\n",
       " ['영화', '보', '고', '싶', '다'],\n",
       " ['요즘', '볼만', '하', 'ㄴ', '거', '있', '어', '?'],\n",
       " ['영화', '나', '예능', '추천'],\n",
       " ['재밌', '는', '드라마', '보여주', '어'],\n",
       " ['신', '과', '함께', '줄거리', '좀', '알려주', '어'],\n",
       " ['고등', '랩', '푸', '어', '다시', '보', '기', '좀'],\n",
       " ['재밌', '는', '영상', '하이라이트', '만', '보여주', '어']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, '배고프': 1, '다': 2, '밥': 3, '주': 4, '어': 5, '뭐': 6, '먹': 7, '을': 8, '만하': 9, 'ㄴ': 10, '거': 11, '없': 12, '냐': 13, '맛': 14, '집': 15, '추천': 16, '이': 17, '근처': 18, '맛있': 19, '는': 20, '음식': 21, '점': 22, '좀': 23, '지': 24, '?': 25, '삼겹살': 26, '고': 27, '싶': 28, '영화': 29, '보': 30, '요즘': 31, '볼만': 32, '하': 33, '있': 34, '나': 35, '예능': 36, '재밌': 37, '드라마': 38, '보여주': 39, '신': 40, '과': 41, '함께': 42, '줄거리': 43, '알려주': 44, '고등': 45, '랩': 46, '푸': 47, '다시': 48, '기': 49, '영상': 50, '하이라이트': 51, '만': 52}\n",
      "{'FOOD': 0, 'MEDIA': 1}\n"
     ]
    }
   ],
   "source": [
    "word2index={'<unk>' : 0}\n",
    "for x in train_X:\n",
    "    for token in x:\n",
    "        if word2index.get(token)==None:\n",
    "            word2index[token]=len(word2index)\n",
    "            \n",
    "class2index = {'FOOD' : 0, 'MEDIA' : 1}\n",
    "print(word2index)\n",
    "print(class2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 26)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index.get(\"패스트\"), word2index.get(\"삼겹살\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_BoW(seq, word2index):\n",
    "    tensor = torch.zeros(len(word2index))\n",
    "    for w in seq:\n",
    "        index = word2index.get(w)\n",
    "        if index!=None:\n",
    "            tensor[index]+=1.\n",
    "        else:\n",
    "            index = word2index['<unk>']\n",
    "            tensor[index]+=1.\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.cat([Variable(make_BoW(x,word2index)).view(1,-1) for x in train_X])\n",
    "train_y = torch.cat([Variable(torch.LongTensor([class2index[y]])) for y in train_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 53]) torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "print(train_X.size(), train_y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## view(*args) → Tensor\n",
    "### Returns a new tensor with the same data as the self tensor but of a different size.\n",
    "\n",
    "The returned tensor shares the same data and must have the same number of elements, <br/>\n",
    "but may have a different size. <br/>\n",
    "For a tensor to be viewed, the new view size must be compatible with its original size and stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x.size())\n",
    "\n",
    "y = x.view(16)\n",
    "print(y.size())\n",
    "\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(z.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 53])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self,vocab_size,output_size):\n",
    "        super(BoWClassifier,self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(vocab_size,output_size)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 100\n",
    "LR = 0.1\n",
    "model = BoWClassifier(len(word2index),2)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7514253854751587\n",
      "0.5621613264083862\n",
      "0.44524627923965454\n",
      "0.36636489629745483\n",
      "0.30981746315956116\n",
      "0.267465204000473\n",
      "0.2346717119216919\n",
      "0.20860494673252106\n",
      "0.18743959069252014\n",
      "0.169948011636734\n"
     ]
    }
   ],
   "source": [
    "for step in range(STEP):\n",
    "    model.zero_grad()\n",
    "    preds = model(train_X)\n",
    "    loss = loss_function(preds,train_y)\n",
    "    if step % 10 == 0:\n",
    "        print(loss.data[0])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2class = {v:k for k,v in class2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : 쭈꾸미 맛집 좀 찾아줘\n",
      "Prediction : FOOD\n",
      "Truth : FOOD\n",
      "\n",
      "\n",
      "Input : 매콤한 떡볶이 먹고싶다\n",
      "Prediction : FOOD\n",
      "Truth : FOOD\n",
      "\n",
      "\n",
      "Input : 강남 씨지비 조조 영화 스케줄표 좀\n",
      "Prediction : MEDIA\n",
      "Truth : MEDIA\n",
      "\n",
      "\n",
      "Input : 효리네 민박 보고싶엉\n",
      "Prediction : MEDIA\n",
      "Truth : MEDIA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in test_data:\n",
    "    X = kor_tagger.morphs(test[0])\n",
    "    X = Variable(make_BoW(X,word2index)).view(1,-1)\n",
    "    \n",
    "    pred = model(X)\n",
    "    pred = pred.max(1)[1].data[0]\n",
    "    print(\"Input : %s\" % test[0])\n",
    "    print(\"Prediction : %s\" % index2class[pred])\n",
    "    print(\"Truth : %s\" % test[1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
