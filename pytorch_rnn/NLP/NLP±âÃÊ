Tokenize
토큰(token)이란 의미를 가지는 문자열을 뜻합니다. 
토큰은 형태소(뜻을 가진 최소 단위)나 그보다 상위 개념인 단어(자립하여 쓸 수 있는 최소 단위)까지 포함합니다. 
토크나이징(tokenizing)이란 문서나 문장을 분석하기 좋도록 토큰으로 나누는 작업입니다. 
영어의 경우 공백만으로도 충분히 토큰을 나눌 수 있다고 합니다. 
물론 ‘-‘(state-of-the-art vs state of the art)이나 합성어(Barack Obama) 등 처리를 세밀히 해줘야 하는 문제가 남아있긴 하지만요. 
띄어쓰기를 거의 하지 않는 중국어의 경우 토큰 분석 작업이 난제로 꼽힙니다.

Morphological analysis
Text Normaization이라고 불리기도 합니다. 
토큰들을 좀 더 일반적인 형태로 분석해 단어수를 줄여 분석의 효율성을 높이는 작업입니다. 
예컨대 ‘cars’와 ‘car’, ‘stopped’와 ‘stop’을 하나의 단어로 본다던지 하는 식입니다. 
영어에선 대문자를 소문자로 바꿔주는 folding도 이와 관련된 중요한 작업이라고 하네요. 
stemming과 lemmatization이라는 작업도 있습니다. stemming이란 단어를 축약형으로 바꿔주는 걸 뜻하고, 
lemmatization은 품사 정보가 보존된 형태의 기본형으로 변환하는 걸 말합니다. 아래 표가 두 기법 차이를 잘 드러내고 있습니다.

Part-Of-Speech Tagging
포스태깅이란 토큰의 품사정보를 할당하는 작업입니다. 
지금까지 많은 방법론들이 개발되었는데요. 
의사결정나무(Decision Trees), 은닉 마코프 모델(Hidden Markov Models), 서포트벡터머신(Support Vector Machines) 
등이 여기에 해당합니다. KoNLPy 같은 한국어 기반의 포스태거들은 문장분리, 토크나이즈, lemmatization, 포스태깅에 
이르기까지 전 과정을 한꺼번에 수행해 줍니다.
하지만 조사, 어미가 발달한 한국어의 경우 정확한 분석이 정말 어렵습니다. 
한국어는 교착어 성질을 지니는 언어이기 때문입니다. 즉 어근에 파생접사나 어미가 붙어서 단어를 이룹니다. 
바꿔 말하면 한국어를 분석할 때 어근과 접사, 어미를 적절하게 나누어야 하는데, 이게 쉽지 않다는 얘기입니다. 
예를 들면 이렇습니다.

깨뜨리시었겠더군요.
여기에서 ‘깨-‘는 어근이며, ‘-뜨리-‘는 접사로서 ‘힘줌’의 뜻을 나타냅니다. 
‘-시-‘는 높임, ‘-었-‘, ‘-겠-‘, ‘-더-‘는 모두 시간을 보이는 어미들이며, ‘-군-‘은 감탄의 뜻을 나타내는 어미, 
‘-요’는 문장을 끝맺는 어미입니다. 위 문장을 제대로 분석하려면 8개 형태소로 쪼개야 합니다. 
매우 고된 작업이지요.



